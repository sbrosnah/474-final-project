{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/spencerbrosnahan/Documents/MyFiles/School/WINTER SEMESTER 2023/CS 474/Final Project/474-final-project/data_prep'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.join(os.getcwd(), \"data_prep\"))\n",
    "sys.path.insert(0, os.path.join(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencerbrosnahan/opt/anaconda3/envs/cs472-env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/spencerbrosnahan/opt/anaconda3/envs/cs472-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "from europarl_data import EuroParlData\n",
    "from transformer import TransformerModel\n",
    "from mt_transformer_trainer import DataLoader, MtTransformerTrainer\n",
    "from bert_embedder import BertEmbedder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data_holder = EuroParlData(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_list = data_holder.get_language_list(\"en\")\n",
    "es_list = data_holder.get_language_list(\"es\")[:100]\n",
    "\n",
    "# data_loader = DataLoader()\n",
    "# SRC, TGT, train, val = data_loader.prep_data(en_list, es_list)\n",
    "\n",
    "# pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "# model = TransformerModel(len(SRC.vocab), len(TGT.vocab), N=2).cuda()\n",
    "# vocab_size = len(TGT.vocab)\n",
    "\n",
    "# trainer = MtTransformerTrainer()\n",
    "# trainer.train_model(model, pad_idx, train, val, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embedder = BertEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done!\n",
      "1% done!\n",
      "2% done!\n",
      "3% done!\n",
      "4% done!\n",
      "5% done!\n",
      "6% done!\n",
      "7% done!\n",
      "8% done!\n",
      "9% done!\n",
      "10% done!\n",
      "11% done!\n",
      "12% done!\n",
      "13% done!\n",
      "14% done!\n",
      "15% done!\n",
      "16% done!\n",
      "17% done!\n",
      "18% done!\n",
      "19% done!\n",
      "20% done!\n",
      "21% done!\n",
      "22% done!\n",
      "23% done!\n",
      "24% done!\n",
      "25% done!\n",
      "26% done!\n",
      "27% done!\n",
      "28% done!\n",
      "29% done!\n",
      "30% done!\n",
      "31% done!\n",
      "32% done!\n",
      "33% done!\n",
      "34% done!\n",
      "35% done!\n",
      "36% done!\n",
      "37% done!\n",
      "38% done!\n",
      "39% done!\n",
      "40% done!\n",
      "41% done!\n",
      "42% done!\n",
      "43% done!\n",
      "44% done!\n",
      "45% done!\n",
      "46% done!\n",
      "47% done!\n",
      "48% done!\n",
      "49% done!\n",
      "50% done!\n",
      "51% done!\n",
      "52% done!\n",
      "53% done!\n",
      "54% done!\n",
      "55% done!\n",
      "56% done!\n",
      "57% done!\n",
      "58% done!\n",
      "59% done!\n",
      "60% done!\n",
      "61% done!\n",
      "62% done!\n",
      "63% done!\n",
      "64% done!\n",
      "65% done!\n",
      "66% done!\n",
      "67% done!\n",
      "68% done!\n",
      "69% done!\n",
      "70% done!\n",
      "71% done!\n",
      "72% done!\n",
      "73% done!\n",
      "74% done!\n",
      "75% done!\n",
      "76% done!\n",
      "77% done!\n",
      "78% done!\n",
      "79% done!\n",
      "80% done!\n",
      "81% done!\n",
      "82% done!\n",
      "83% done!\n",
      "84% done!\n",
      "85% done!\n",
      "86% done!\n",
      "87% done!\n",
      "88% done!\n",
      "89% done!\n",
      "90% done!\n",
      "91% done!\n",
      "92% done!\n",
      "93% done!\n",
      "94% done!\n",
      "95% done!\n",
      "96% done!\n",
      "97% done!\n",
      "98% done!\n",
      "99% done!\n"
     ]
    }
   ],
   "source": [
    "en_embeddings = embedder.get_sentence_embeddings(es_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_states shape: torch.Size([5, 49, 768])\n",
      "last_hidden_states shape: torch.Size([5, 91, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 5\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, 10, batch_size):\n",
    "    batch_sentences = es_list[i:i+batch_size]   \n",
    "    \n",
    "    batch_tokens = tokenizer.batch_encode_plus(batch_sentences, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], token_type_ids=batch_tokens['token_type_ids'])\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(\"last_hidden_states shape:\", last_hidden_states.shape)\n",
    "\n",
    "    # # Calculate the average of all 23 token vectors.\n",
    "    sentence_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    embeddings.append(sentence_embeddings)\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.ones(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ex, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs472-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c5dc66d2449d7ef5b491873d284acbabe4298817661168b15616f78616d743e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
